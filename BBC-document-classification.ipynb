{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abefe1a-735f-400c-b997-466326e9c19e",
   "metadata": {},
   "source": [
    "# BBC News Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3136d9b8-3653-4c3e-b8b2-6edbcf144870",
   "metadata": {},
   "source": [
    "Useful Resources\n",
    "- https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\n",
    "- https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/\n",
    "- https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n",
    "- http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "- https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245efb97-dabb-4ab4-9830-e847f1a7177e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3c92cc6-df40-4f43-8fa7-f3d0a2f5e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import gensim\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    plot_confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb56f7e-3563-40f0-9c08-4dec6c48d61b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Fetch sub-directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d92ef9-4ddf-4f58-8c3d-d20c84803d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subfolder_names(path, excluded_files):\n",
    "    return [file for file in os.listdir(path) if file not in excluded_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83774ca1-f1c6-4669-9c2b-75c84221a9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_folders = get_subfolder_names('D:/Resume-projects/nlp-data/document-classification/bbc','README.TXT')\n",
    "sub_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc91640-c981-4ffa-b7a9-3c24c3d73054",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Fetch sub-directory wise text file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f58632-0f95-4447-b777-18a8f8269ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_folderwise_txtfile_names(path, folder_names):\n",
    "    txtfile_dict = {}\n",
    "    for folder in folder_names:\n",
    "        txtfile_dict[folder] = [file for file in os.listdir(path + folder)]\n",
    "    return txtfile_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95000609-c639-498d-aca9-172be70f9961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001.txt',\n",
       " '002.txt',\n",
       " '003.txt',\n",
       " '004.txt',\n",
       " '005.txt',\n",
       " '006.txt',\n",
       " '007.txt',\n",
       " '008.txt',\n",
       " '009.txt',\n",
       " '010.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtfiles_dict = get_folderwise_txtfile_names('D:/Resume-projects/nlp-data/document-classification/bbc/',sub_folders)\n",
    "txtfiles_dict['tech'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64193d3a-743d-443c-a43d-4ede26554cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all extracted subfolders and present in the dictionary? True\n"
     ]
    }
   ],
   "source": [
    "print('Are all extracted subfolders and present in the dictionary? ' + str(sub_folders == list(txtfiles_dict.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3d9fe-5476-4472-889a-7a54b472e6b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creating dataframe with two columns (article as text and its corresponding category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3daafe03-659d-4148-9f57-594ba8f98895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['article', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fadc8a-c4ad-4176-8422-541f48a610be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(path, folder_txtfile_dict, df):\n",
    "    for folder, txt_filenames in folder_txtfile_dict.items():\n",
    "        for filename in txt_filenames:\n",
    "            file = open(path + folder + '/' + filename)\n",
    "            new_row = {'article': file.read().replace(\"\\n\", \" \").strip(), 'category': folder}\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de75fd89-f30c-4168-b615-c87f0f573935",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df('D:/Resume-projects/nlp-data/document-classification/bbc/', txtfiles_dict, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11632a8e-857c-4c8b-b48a-8dc337b8143c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b993774b-eed0-4796-88cc-9def3c39eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation(df, text_col):\n",
    "    df[text_col] = df[text_col].str.replace('$', '$\\$$')  # Prevent latex styling due to dollar sign\n",
    "    df = df.sample(frac=1, random_state=7)  # Resampling data\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55308408-55c6-4129-af8d-89115bfdbffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THEDEV~1\\AppData\\Local\\Temp/ipykernel_15200/1460542178.py:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df[text_col] = df[text_col].str.replace('$', '$\\$$')  # Prevent latex styling due to dollar sign\n"
     ]
    }
   ],
   "source": [
    "df = data_manipulation(df, 'article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac39fae-8c7c-4135-911c-88d2108ab4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessor(text):\n",
    "    filtered_sentences = []\n",
    "    lemma_word = []\n",
    "    stop_words = set(stopwords.words('english') + ['\\'s'])\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    text = re.sub(pattern='[^\\w\\s]', repl = '', string = text)\n",
    "    # Remove stop words\n",
    "    word_tokens = word_tokenize(text)\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentences.append(w)\n",
    "    # Lemmatization\n",
    "    for w in filtered_sentences:\n",
    "        word1 = wordnet_lemmatizer.lemmatize(w, pos=\"n\")\n",
    "        word2 = wordnet_lemmatizer.lemmatize(word1, pos=\"v\")\n",
    "        word3 = wordnet_lemmatizer.lemmatize(word2, pos=(\"a\"))\n",
    "        lemma_word.append(word3)\n",
    "    filtered_text = \" \".join(lemma_word)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7419ec42-eea9-4d75-8cc1-4db5898e8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article'] = df['article'].apply(text_preprocessor)  # converts sentence to lower case and removes stop words, punctuations and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5603864-8e32-4d22-95ff-e97451eac30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2225 articles from BBC in this dataset with categories belonging to business, entertainment, politics, sport and tech\n"
     ]
    }
   ],
   "source": [
    "print('There are '+ str(df.shape[0]) + ' articles from BBC in this dataset with categories belonging to', \\\n",
    "      ', '.join([str(elem) for elem in sub_folders[:-1]]) + ' and ' + sub_folders[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8109d8e-57fb-407b-9d9e-14478241cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>howard deny split id card michael howard deny shadow cabinet split decision back controversial labour plan introduce id card tory leader say front bench team reach collective view hold good discussion admit easy issue decide support plan police say would help fight terror crime illegal immigration lib dems pledge oppose bill debate next monday tory source say senior party figure argue vociferously id card scheme among report serious reservation strategy senior shadow cabinet member david davis oliver letwin tim yeo mr howard deny mr yeo transport environment spokesman say plan stink also say confident shadow home secretary mr davis would set position clearly stand debate matter next week mr howard say police say id card could help foil terror bomb plot people could lose life add police say take seriously acknowledge good libertarian argument card say shadow cabinet weigh conflict interest reach decision dont pretend easy decision end day decision take also deny afraid look soft issue compare labour conservative announce support government plan monday even source within party tell bbc mr howard always favour id card try introduce home secretary tory insist would hold minister account precise purpose scheme say would also press labour whether objective could meet whether home office would able deliver pledge ass cost effectiveness id card whether people privacy would properly protect important remember bill take decade come full effect spokesman say lib dem home affair spokesman mark oaten brand id scheme waste money deeply flaw say sign michael howard overrule colleague concern id card chairman bar council guy mansfield qc warn real risk people margin society would drive hand extremist go happen young asian men bomb go somewhere go stop havent id card go detain tory exminister douglas hogg say oppose plan id card brand regressive step would intrude life ordinary citizen without counterbalance benefit predict ultimately carry card would become compulsory would lead large number britain ethnic minority stop police</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>quiksilver move rossignol share ski rossignol world large skimaker jump much 15 speculation buy u surfwear firm quiksilver owner rossignol boixvives family say consider offer quiksilver analyst believe sport good company may take close look rossignol prompt auction push sale price high nike k2 previously mention possible suitor rossignol share touch 1770 euro fall back trade 78 high 1660 euro european sport good company see foreign revenue squeeze slump value u dollar make takeover attractive analyst say company quiksilver would able cut cost sell rossignol ski shop add boixvives family think spend past couple year sound possible suitor rossignol also make golf equipment snowboard sport clothe</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>moyes uturn beattie dismissal everton manager david moyes discipline striker jam beattie headbutt chelsea defender william gallas scot initially defend beattie whose dismissal put everton back foot game ultimately lose 10 say gallas overreact rethink look video evidence say believe set record straight concede dismissal right correct moyes add comment saturday come immediately final whistle point opportunity see one quick rerun incident club website also report beattie seem unrepentant saturday match insist gallas would stay lot long headbutt apologise moyes continue although incident totally character jam never even suspend career action unacceptable detrimental effect teammate jam issue formal apology teammate everton supporter immediately game right thing do subject normal club discipline competitive player fair player know upset happen however must say still believe chelsea player question go easily speak immediately game moyes say dont think sendingoff centrehalf time would ashamed go easily million year would john terry go way never hear anybody butt somebody behind run happen big strong centrehalves think push initially still dont think sendingoff angry beattie initially say gallas would stay lot long headbutt tell wasnt intentional headbutt chase ball corner william gallas look shoulder block stop run say youre go stay way ill go straight head barely touch wasnt intentional headbutt</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>anelka eye man city departure striker nicolas anelka reportedly want leave manchester city search champion league football anelka 25 talk contract extension beyond 2007 city believe fear career go stale stay club news world report anelka tell french magazine either decide win title easy life think always choose football great club make offer add win title need player capability stagnate eighth 15th place impossible progress go score goal win risk go create feel feed anelka earn reputation difficult character handle spell arsenal real madrid paris st germain feel come back haunt talk sign extension contract say well sport aspect also come account play eighth place good miss champion league real madrid 2000 need play play im thing happen past nothing football dont blame anyone bite fault</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>amex share spinoff news share american express surge 8 tuesday say spin le profitable financial advisory subsidiary u credit card travel service giant say offload american express financial advisor aefa would boost profitability aefa 12000 adviser sell financial advice fund insurance 25 million customer year deliver poor profit even loss excellent move american express focus core business sell laggard division problem quite time say marquis investment research analyst phil kain analyst estimate standalone aefa could market value 10bn â53bn unit acquire american express 20 year ago investor diversify service minneapolis time firm amass onestop financial empire however business sell investment never integrate rest group</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             article  \\\n",
       "1036  howard deny split id card michael howard deny shadow cabinet split decision back controversial labour plan introduce id card tory leader say front bench team reach collective view hold good discussion admit easy issue decide support plan police say would help fight terror crime illegal immigration lib dems pledge oppose bill debate next monday tory source say senior party figure argue vociferously id card scheme among report serious reservation strategy senior shadow cabinet member david davis oliver letwin tim yeo mr howard deny mr yeo transport environment spokesman say plan stink also say confident shadow home secretary mr davis would set position clearly stand debate matter next week mr howard say police say id card could help foil terror bomb plot people could lose life add police say take seriously acknowledge good libertarian argument card say shadow cabinet weigh conflict interest reach decision dont pretend easy decision end day decision take also deny afraid look soft issue compare labour conservative announce support government plan monday even source within party tell bbc mr howard always favour id card try introduce home secretary tory insist would hold minister account precise purpose scheme say would also press labour whether objective could meet whether home office would able deliver pledge ass cost effectiveness id card whether people privacy would properly protect important remember bill take decade come full effect spokesman say lib dem home affair spokesman mark oaten brand id scheme waste money deeply flaw say sign michael howard overrule colleague concern id card chairman bar council guy mansfield qc warn real risk people margin society would drive hand extremist go happen young asian men bomb go somewhere go stop havent id card go detain tory exminister douglas hogg say oppose plan id card brand regressive step would intrude life ordinary citizen without counterbalance benefit predict ultimately carry card would become compulsory would lead large number britain ethnic minority stop police   \n",
       "372                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   quiksilver move rossignol share ski rossignol world large skimaker jump much 15 speculation buy u surfwear firm quiksilver owner rossignol boixvives family say consider offer quiksilver analyst believe sport good company may take close look rossignol prompt auction push sale price high nike k2 previously mention possible suitor rossignol share touch 1770 euro fall back trade 78 high 1660 euro european sport good company see foreign revenue squeeze slump value u dollar make takeover attractive analyst say company quiksilver would able cut cost sell rossignol ski shop add boixvives family think spend past couple year sound possible suitor rossignol also make golf equipment snowboard sport clothe   \n",
       "1409                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            moyes uturn beattie dismissal everton manager david moyes discipline striker jam beattie headbutt chelsea defender william gallas scot initially defend beattie whose dismissal put everton back foot game ultimately lose 10 say gallas overreact rethink look video evidence say believe set record straight concede dismissal right correct moyes add comment saturday come immediately final whistle point opportunity see one quick rerun incident club website also report beattie seem unrepentant saturday match insist gallas would stay lot long headbutt apologise moyes continue although incident totally character jam never even suspend career action unacceptable detrimental effect teammate jam issue formal apology teammate everton supporter immediately game right thing do subject normal club discipline competitive player fair player know upset happen however must say still believe chelsea player question go easily speak immediately game moyes say dont think sendingoff centrehalf time would ashamed go easily million year would john terry go way never hear anybody butt somebody behind run happen big strong centrehalves think push initially still dont think sendingoff angry beattie initially say gallas would stay lot long headbutt tell wasnt intentional headbutt chase ball corner william gallas look shoulder block stop run say youre go stay way ill go straight head barely touch wasnt intentional headbutt   \n",
       "1518                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    anelka eye man city departure striker nicolas anelka reportedly want leave manchester city search champion league football anelka 25 talk contract extension beyond 2007 city believe fear career go stale stay club news world report anelka tell french magazine either decide win title easy life think always choose football great club make offer add win title need player capability stagnate eighth 15th place impossible progress go score goal win risk go create feel feed anelka earn reputation difficult character handle spell arsenal real madrid paris st germain feel come back haunt talk sign extension contract say well sport aspect also come account play eighth place good miss champion league real madrid 2000 need play play im thing happen past nothing football dont blame anyone bite fault   \n",
       "279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          amex share spinoff news share american express surge 8 tuesday say spin le profitable financial advisory subsidiary u credit card travel service giant say offload american express financial advisor aefa would boost profitability aefa 12000 adviser sell financial advice fund insurance 25 million customer year deliver poor profit even loss excellent move american express focus core business sell laggard division problem quite time say marquis investment research analyst phil kain analyst estimate standalone aefa could market value 10bn â53bn unit acquire american express 20 year ago investor diversify service minneapolis time firm amass onestop financial empire however business sell investment never integrate rest group   \n",
       "\n",
       "      category  \n",
       "1036  politics  \n",
       "372   business  \n",
       "1409     sport  \n",
       "1518     sport  \n",
       "279   business  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b4cd2c8-9059-49be-9c4d-9dd58e7cf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "# for i in symbols:\n",
    "#     data = np.char.replace(data, i, ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8cf0cc-f592-4759-a984-f54bb6a7f713",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7d75f45-2bea-448f-b468-d32290166411",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['article']\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d0a77da-8f20-412c-b59b-11075a4b1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1668,) (557,) (1668,) (557,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9988fe-270a-4bee-89fe-990a17395e63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad8ee4d-b767-43e7-a54b-7c2a5f4e61c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41b1fa94-c892-449d-8373-1e7f34e6752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "cv = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8876bf7d-8b90-4614-8b5c-5388ddc8ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_op = pd.DataFrame(data=cv.toarray(), index=X_train.index ,columns = count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089db45d-298f-4721-98fc-1a6146d0a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THEDEV~1\\AppData\\Local\\Temp/ipykernel_15200/3755401756.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  count_vec_df['no_of_words_cv'] = count_vec_df.sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004secs</th>\n",
       "      <th>007</th>\n",
       "      <th>01</th>\n",
       "      <th>...</th>\n",
       "      <th>â960m</th>\n",
       "      <th>â96bn</th>\n",
       "      <th>â97m</th>\n",
       "      <th>â980m</th>\n",
       "      <th>â98m</th>\n",
       "      <th>â99</th>\n",
       "      <th>â99m</th>\n",
       "      <th>â9m</th>\n",
       "      <th>no_of_words_cv</th>\n",
       "      <th>no_of_words_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>342</td>\n",
       "      <td>396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>sport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>313</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category  00  000  0001  001  002  003  004secs  007  01  ...  \\\n",
       "1237       politics   0    0     0    0    0    0        0    0   0  ...   \n",
       "631   entertainment   0    0     0    0    0    0        0    0   0  ...   \n",
       "1641          sport   0    0     0    0    0    0        0    0   0  ...   \n",
       "1015       politics   0    0     0    0    0    0        0    0   0  ...   \n",
       "772   entertainment   0    0     0    0    0    0        0    0   0  ...   \n",
       "\n",
       "      â960m  â96bn  â97m  â980m  â98m  â99  â99m  â9m  no_of_words_cv  \\\n",
       "1237      0      0     0      0     0    0     0    0             187   \n",
       "631       0      0     0      0     0    0     0    0             342   \n",
       "1641      0      0     0      0     0    0     0    0             228   \n",
       "1015      0      0     0      0     0    0     0    0             205   \n",
       "772       0      0     0      0     0    0     0    0             313   \n",
       "\n",
       "      no_of_words_txt  \n",
       "1237              212  \n",
       "631               396  \n",
       "1641              262  \n",
       "1015              222  \n",
       "772               351  \n",
       "\n",
       "[5 rows x 23195 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_df = pd.concat([pd.DataFrame(y_train), count_vec_op], axis=1, ignore_index=False)\n",
    "count_vec_df['no_of_words_cv'] = count_vec_df.sum(axis=1)\n",
    "count_vec_df['no_of_words_txt'] = df['article'].apply(lambda x: len(x.split(' ')))\n",
    "count_vec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e88ec00-c5b9-4d5e-a99c-b1ce39aea7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec_df = count_vec_df.drop(['no_of_words_cv','no_of_words_txt'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55ea0a22-4c12-4bdf-ad8c-590f0ff04676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004secs</th>\n",
       "      <th>007</th>\n",
       "      <th>01</th>\n",
       "      <th>...</th>\n",
       "      <th>â958m</th>\n",
       "      <th>â95m</th>\n",
       "      <th>â960m</th>\n",
       "      <th>â96bn</th>\n",
       "      <th>â97m</th>\n",
       "      <th>â980m</th>\n",
       "      <th>â98m</th>\n",
       "      <th>â99</th>\n",
       "      <th>â99m</th>\n",
       "      <th>â9m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>sport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>politics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category  00  000  0001  001  002  003  004secs  007  01  ...  \\\n",
       "1237       politics   0    0     0    0    0    0        0    0   0  ...   \n",
       "631   entertainment   0    0     0    0    0    0        0    0   0  ...   \n",
       "1641          sport   0    0     0    0    0    0        0    0   0  ...   \n",
       "1015       politics   0    0     0    0    0    0        0    0   0  ...   \n",
       "772   entertainment   0    0     0    0    0    0        0    0   0  ...   \n",
       "\n",
       "      â958m  â95m  â960m  â96bn  â97m  â980m  â98m  â99  â99m  â9m  \n",
       "1237      0     0      0      0     0      0     0    0     0    0  \n",
       "631       0     0      0      0     0      0     0    0     0    0  \n",
       "1641      0     0      0      0     0      0     0    0     0    0  \n",
       "1015      0     0      0      0     0      0     0    0     0    0  \n",
       "772       0     0      0      0     0      0     0    0     0    0  \n",
       "\n",
       "[5 rows x 23193 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e91b9-d89a-4bf7-8395-ae668c8da8b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "077f309c-4332-4cd8-a735-6811c7f65487",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidfv = tfidf_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbb21184-d3c3-4334-b7d7-d1dbe8c25a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec_op = pd.DataFrame(data=tfidfv.toarray(), index=X_train.index ,columns = tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c93bd60-eb8e-43b0-bf11-766a425b48d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0001</th>\n",
       "      <th>001</th>\n",
       "      <th>002</th>\n",
       "      <th>003</th>\n",
       "      <th>004secs</th>\n",
       "      <th>007</th>\n",
       "      <th>01</th>\n",
       "      <th>...</th>\n",
       "      <th>â958m</th>\n",
       "      <th>â95m</th>\n",
       "      <th>â960m</th>\n",
       "      <th>â96bn</th>\n",
       "      <th>â97m</th>\n",
       "      <th>â980m</th>\n",
       "      <th>â98m</th>\n",
       "      <th>â99</th>\n",
       "      <th>â99m</th>\n",
       "      <th>â9m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>politics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>sport</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>politics</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category   00  000  0001  001  002  003  004secs  007   01  ...  \\\n",
       "1237       politics  0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0  ...   \n",
       "631   entertainment  0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0  ...   \n",
       "1641          sport  0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0  ...   \n",
       "1015       politics  0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0  ...   \n",
       "772   entertainment  0.0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0  ...   \n",
       "\n",
       "      â958m  â95m  â960m  â96bn  â97m  â980m  â98m  â99  â99m  â9m  \n",
       "1237    0.0   0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0  \n",
       "631     0.0   0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0  \n",
       "1641    0.0   0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0  \n",
       "1015    0.0   0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0  \n",
       "772     0.0   0.0    0.0    0.0   0.0    0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 23193 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec_df = pd.concat([pd.DataFrame(y_train), tfidf_vec_op], axis=1, ignore_index=False)\n",
    "tfidf_vec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6b934-c55e-450f-a525-a21f35636e27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae7c6f02-fbe3-42a0-a327-521bd25aa3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CBOW model\n",
    "w2v_cbow = gensim.models.Word2Vec(X_train, min_count = 1, window = 5)\n",
    "# # Print results\n",
    "# print(\"Cosine similarity between 'alice' \" + \n",
    "#                \"and 'wonderland' - CBOW : \",\n",
    "#     model1.similarity('alice', 'wonderland'))\n",
    "      \n",
    "# print(\"Cosine similarity between 'alice' \" +\n",
    "#                  \"and 'machines' - CBOW : \",\n",
    "#       model1.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b79d5321-8266-462c-a949-a95c17e4590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1b1387270d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8dea8b-727a-44e8-8c11-8c52ed26bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Skip Gram model\n",
    "w2v_skp_grm = gensim.models.Word2Vec(X_train, min_count = 1, size = 100, window = 5, sg = 1)\n",
    "# # Print results\n",
    "# print(\"Cosine similarity between 'alice' \" +\n",
    "#           \"and 'wonderland' - Skip Gram : \",\n",
    "#     model2.similarity('alice', 'wonderland'))\n",
    "      \n",
    "# print(\"Cosine similarity between 'alice' \" +\n",
    "#             \"and 'machines' - Skip Gram : \",\n",
    "#       model2.similarity('alice', 'machines'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322e1b00-5781-4927-b691-44ef11038010",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd642732-c39a-46a4-806f-33627413e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Returns mean and std of cross validation\n",
    "#     \"\"\"\n",
    "#     scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "#     mean_scores = pd.DataFrame(scores).mean()\n",
    "#     std_scores = pd.DataFrame(scores).std()\n",
    "#     out_col = []\n",
    "\n",
    "#     for i in range(len(mean_scores)):\n",
    "#         out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "#     return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278fe28-7edc-46e4-b9fc-4dc1fed18248",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dummy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dbc5f6-9a4e-4f3b-afb1-72a277a74c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# dummy = DummyClassifier()\n",
    "# results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "#     dummy, X_train, y_train, return_train_score=True\n",
    "# )\n",
    "# pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3241a9-daef-4cc9-952e-52088fd9b2f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee584a-4b02-4d16-8ffa-7205d7faf0e0",
   "metadata": {},
   "source": [
    "### fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea15a2-53db-490f-b4dd-ab2e2874642d",
   "metadata": {},
   "source": [
    "### BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc6767-b190-4c34-92ef-bcb363e21ab4",
   "metadata": {},
   "source": [
    "### RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da72ace-d178-4887-8e98-fde73eeec81a",
   "metadata": {},
   "source": [
    "### AlBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a8c2d-b998-409c-af2c-4e9b9e913c40",
   "metadata": {},
   "source": [
    "### DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1445e-f9de-4479-8930-ce1758d0d87b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenAI Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a53b3a-0dbb-42e7-b8c7-9218daf90712",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7174a88-8aff-4575-a128-cc0f94e65f00",
   "metadata": {},
   "source": [
    "### ULM-Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e94b8-aea1-4523-ab2d-1bd163146a5c",
   "metadata": {},
   "source": [
    "### T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e456c9-aa1d-4454-9b97-90acb4d9e8af",
   "metadata": {},
   "source": [
    "### PEGASUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c51e1-a748-46b2-aad5-24a8d42ab675",
   "metadata": {},
   "source": [
    "### XLNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42268837-6755-46a2-9d24-218bcb499a99",
   "metadata": {},
   "source": [
    "### Reformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533a8c3-9bc6-4e0c-aaa1-e42c624f4c9a",
   "metadata": {},
   "source": [
    "### MT-DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f308ac8f-31a3-4ce1-8ec1-18aa4fa9b13f",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84cb730-fc91-42cd-b2ba-fea5ac1c260b",
   "metadata": {},
   "source": [
    "### GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f635470b-08a1-4e14-884c-aa37f7c452a4",
   "metadata": {},
   "source": [
    "### GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11493ca8-66ad-4543-92c0-9032db68a3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
